{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d583a0-13d7-426e-ae62-4070137f62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "import json\n",
    "\n",
    "IS_FOR_PAPER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27fea4-801d-4d2e-a92e-a491af7f44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with closing(sqlite3.connect(\"../db/remote-simulations-sadegh.db\")) as connection:\n",
    "    with closing(connection.cursor()) as cursor:\n",
    "        rows = cursor.execute(\"SELECT * from results\").fetchall()\n",
    "        print(rows)\n",
    "'''\n",
    "with closing(sqlite3.connect(\"../db/remote-simulations-sadegh.db\")) as connection_sadegh, \\\n",
    "        closing(sqlite3.connect(\"../db/remote-simulations.db\")) as connection_up, \\\n",
    "        closing(sqlite3.connect(\"../db/remote-simulations-vmware.db\")) as connection_vmware:\n",
    "    sql_query = pd.read_sql_query(\"SELECT * from results\", connection_sadegh)\n",
    "    results_sadegh = pd.DataFrame(sql_query)\n",
    "    sql_query = pd.read_sql_query(\"SELECT * from results\", connection_up)\n",
    "    results_up = pd.DataFrame(sql_query)\n",
    "    sql_query = pd.read_sql_query(\"SELECT * from results\", connection_vmware)\n",
    "    results_vmware = pd.DataFrame(sql_query)\n",
    "\n",
    "results = pd.concat([results_sadegh, results_up, results_vmware])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d66695-6df5-4740-95ec-dc6bb8ea995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import functools\n",
    "\n",
    "COMPARE_PROPS = ['row_lower_bound', 'row_upper_bound', 'column_lower_bound', 'column_upper_bound']\n",
    "#COMPARE_PROPS = ['dataset']\n",
    "#COMPARE_PROPS = ['dataset', 'metric_name']\n",
    "#COMPARE_PROPS = ['services_count']\n",
    "\n",
    "props_values = results[COMPARE_PROPS].value_counts().index.to_frame(index=False)\n",
    "params = {\n",
    "    'NODES_COUNT': 7,\n",
    "    'SERVICES_COUNT': 5,\n",
    "    'METRIC_NAME': 'qualitative',\n",
    "    #'ROW_LOWER_BOUND': 0.2,\n",
    "    #'ROW_UPPER_BOUND': 1,\n",
    "    #'COLUMN_LOWER_BOUND': 0.6,\n",
    "    #'COLUMN_UPPER_BOUND': 0.9,\n",
    "    'DATASET': 'red_wine_quality',\n",
    "    'DESCRIPTION': 'column and row filtering based on categories - boundaries fixed',\n",
    "    'FILTERING_TYPE': 'mixed'\n",
    "}\n",
    "\n",
    "results_filters = [\n",
    "    lambda df: df['services_count'] == params['SERVICES_COUNT'],\n",
    "    lambda df: df['nodes_count'] == params['NODES_COUNT'],\n",
    "    lambda df: df['metric_name'] == params['METRIC_NAME'],\n",
    "    #lambda df: df['row_lower_bound'] == params['ROW_LOWER_BOUND'],\n",
    "    #lambda df: df['row_upper_bound'] == params['ROW_UPPER_BOUND'],\n",
    "    #lambda df: df['column_lower_bound'] == params['COLUMN_LOWER_BOUND'],\n",
    "    #lambda df: df['column_upper_bound'] == params['COLUMN_UPPER_BOUND'],\n",
    "    lambda df: df['dataset'] == params['DATASET'],\n",
    "    lambda df: df['description'] == params['DESCRIPTION'],\n",
    "    lambda df: df['filtering_type'] == params['FILTERING_TYPE'],\n",
    "]\n",
    "\n",
    "def get_compare_props_filters(p_values):\n",
    "    result = []\n",
    "    for field in p_values._fields:\n",
    "        field_value = getattr(p_values, field)\n",
    "        result.append((lambda f, fv: lambda df: df[f] == fv)(field, field_value))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def apply_filters_to_dataset(df, filters):\n",
    "    compound_condition = functools.reduce(operator.and_, map(lambda filter: filter(df), filters))\n",
    "    return df[compound_condition]\n",
    "\n",
    "props_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43edb0d6-5650-4c2d-98f5-77bf274411a9",
   "metadata": {},
   "source": [
    "## Aggregate results by multiple dimension\n",
    "\n",
    "After filtering the results, they are grouped by `window_size` so that metric values with different `experiment_id` are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f853e-467d-4356-981e-3b075589e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = {}\n",
    "        \n",
    "for p_values in props_values.itertuples(index=False):\n",
    "    filtered_results = apply_filters_to_dataset(results, results_filters + get_compare_props_filters(p_values)) \\\n",
    "        .groupby(['window_size'])[['metric_value']].mean() \\\n",
    "        .sort_values(by=['window_size', 'metric_value'])\n",
    "\n",
    "    display(filtered_results)\n",
    "    agg_results[p_values] = filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16c0ea-3b08-491d-9915-e429d229f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_aggregated_results(agg_results):\n",
    "    return { str(k): v.to_dict() for k, v in agg_results.items() }\n",
    "    \n",
    "with open('saved_agg.json', 'w') as f:\n",
    "    f.write(json.dumps(save_aggregated_results(agg_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc2136-d190-48d8-9760-4f5084167f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "for p_values, dresults in sorted(agg_results.items(), key=lambda x: x[0]):\n",
    "    if len(dresults) == 0:\n",
    "        continue\n",
    "    ax.plot(dresults.index, dresults['metric_value'], label=str(p_values)[7:-1])\n",
    "    plt.xticks(dresults.index)\n",
    "\n",
    "plot_title = ', '.join(map(lambda x: f'{x[0]}={x[1]}', params.items()))\n",
    "if not IS_FOR_PAPER:\n",
    "    ax.set_title(plot_title)\n",
    "ax.set_xlabel('Window size')\n",
    "ax.set_ylabel('Metric value')\n",
    "ax.grid(True)\n",
    "\n",
    "if IS_FOR_PAPER:\n",
    "    ax.legend(bbox_to_anchor=(0.5, -0.70), loc='lower center', borderaxespad=0)\n",
    "else:\n",
    "    ax.legend(bbox_to_anchor=(1.02, 0.1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "plt.savefig(f'agg_{\",\".join(COMPARE_PROPS)}.svg', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c48064-5420-45be-808a-06e5e233a1a8",
   "metadata": {},
   "source": [
    "## All experiments by Dataset\n",
    "\n",
    "Results for the same experiment id are not aggregated but have their own plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6162a9-1773-4c91-a292-23b8b64dfe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map = {}\n",
    "for p_values in props_values.itertuples(index=False):\n",
    "    results_map[p_values] = {}\n",
    "    filtered_results = apply_filters_to_dataset(results, results_filters + get_compare_props_filters(p_values))\n",
    "    experiment_ids = filtered_results['experiment_id'].unique()\n",
    "    for exp_id in experiment_ids:\n",
    "        exp_id_results = filtered_results[filtered_results['experiment_id'] == exp_id][['window_size', 'metric_value']].drop_duplicates()\n",
    "        results_map[p_values][exp_id] = exp_id_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cacce-eac3-41ac-b1cd-18709710c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def compute_diff_for_results_per_prop_reducer(acc: List[Tuple[float, float]], cur: float):\n",
    "    '''\n",
    "    acc: (metric difference with the previous window size, metric for the current window size)\n",
    "    '''\n",
    "    metric_diff = cur - acc[-1][1]\n",
    "    return acc + [(metric_diff, cur)]\n",
    "\n",
    "def compute_diff_for_results_per_prop(results_per_prop: Dict[int, pd.DataFrame]) -> List[List[float]]:\n",
    "    return np.array([\n",
    "        np.array(list(map(lambda x: x[0], functools.reduce(compute_diff_for_results_per_prop_reducer, \\\n",
    "                         exp_results.iloc[1:, exp_results.columns.get_loc('metric_value')], \\\n",
    "                         [(0, float(exp_results.iloc[0, exp_results.columns.get_loc('metric_value')]))] \\\n",
    "                        )))) \\\n",
    "        for exp_results in results_per_prop.values()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2420a-052b-44ce-9c2e-067476301546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_results(all_results):\n",
    "    return { str(k): { str(k1): v1.to_dict() for k1, v1 in v.items() } for k, v in all_results.items() }\n",
    "    \n",
    "with open('saved_all.json', 'w') as f:\n",
    "    f.write(json.dumps(save_all_results(results_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edea4e2-bfdb-4439-8154-b4b84c369924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_plot_settings(ax, title):\n",
    "    ax.set_xlabel('Window size')\n",
    "    ax.set_ylabel('Metric value')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    \n",
    "\n",
    "results_map = {k:v for k, v in results_map.items() if len(v) > 0}\n",
    "# 1. a boxplot\n",
    "# 2. a boxplot where elements are the difference between the metric of winsize `x` and winsize `x - 1`\n",
    "# 3. one plot that shows all the plot in the same subfigure\n",
    "additional_plot_count = 3\n",
    "fig_width = 9 if IS_FOR_PAPER else (max(len(exp_res) + additional_plot_count for exp_res in results_map.values()) * 4)\n",
    "\n",
    "results_count = len(results_map)\n",
    "fig_height = results_count * 5\n",
    "fig = plt.figure(figsize=(fig_width, fig_height), constrained_layout=True)\n",
    "\n",
    "plot_title = ', '.join(map(lambda x: f'{x[0]}={x[1]}', params.items()))\n",
    "if not IS_FOR_PAPER:\n",
    "    fig.suptitle(plot_title)\n",
    "\n",
    "subfigs = fig.subfigures(nrows=results_count, ncols=1, squeeze=False)\n",
    "\n",
    "for i, (p_values, results_per_prop) in enumerate(sorted(results_map.items(), key=lambda x: x[0])):\n",
    "    subfig = subfigs.item(i)\n",
    "    subfig.suptitle(f'Results for {str(p_values)[6:]}')\n",
    "\n",
    "    axs_len = 2 if IS_FOR_PAPER else (len(results_per_prop) + additional_plot_count)\n",
    "    axs = subfig.subplots(1, axs_len, sharex=True, squeeze=True)\n",
    "    min_y = min(results_per_prop[k]['metric_value'].min() for k in results_per_prop.keys())\n",
    "    max_y = max(results_per_prop[k]['metric_value'].max() for k in results_per_prop.keys())\n",
    "    # this offset let the plot to be slightly distant from the figure border\n",
    "    y_offset = (max_y - min_y) / 15\n",
    "    for i, ax in enumerate(axs):\n",
    "        if i != 1:\n",
    "            ax.set_ylim(min_y - y_offset, max_y + y_offset)\n",
    "\n",
    "    # additional_plot 1\n",
    "    axs[0].boxplot(np.array([results_per_prop[exp_id]['metric_value'].to_numpy() for exp_id in results_per_prop.keys()]))\n",
    "    prop_agg_results = agg_results[p_values]\n",
    "    axs[0].plot(prop_agg_results.index, prop_agg_results['metric_value'], label=str(p_values)[7:-1])\n",
    "    set_plot_settings(axs[0], f'Boxplot with average')\n",
    "\n",
    "    # additional_plot 2\n",
    "    axs[1].boxplot(compute_diff_for_results_per_prop(results_per_prop))\n",
    "    set_plot_settings(axs[1], f'Boxplot for metric difference between winsizes')\n",
    "\n",
    "    if not IS_FOR_PAPER:\n",
    "        for exp_index, (exp_id, exp_id_results) in enumerate(results_per_prop.items()):\n",
    "            # additional_plot 3\n",
    "            axs[2].plot(exp_id_results['window_size'], exp_id_results['metric_value'])\n",
    "            set_plot_settings(axs[2], f'All experiments')\n",
    "            axs[exp_index + additional_plot_count].plot(exp_id_results['window_size'], exp_id_results['metric_value'])\n",
    "            set_plot_settings(axs[exp_index + additional_plot_count], f'Experiment id = {exp_id}')\n",
    "\n",
    "#fig.tight_layout()\n",
    "plt.savefig(f'all_exp_{\",\".join(COMPARE_PROPS)}.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
