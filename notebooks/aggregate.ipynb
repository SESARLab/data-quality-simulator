{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d583a0-13d7-426e-ae62-4070137f62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "import json\n",
    "\n",
    "IS_FOR_PAPER = True\n",
    "LOAD_FROM_JSON = True\n",
    "AGG_RESULTS_JSON_FILE = '../results/services-range/agg_results.json'\n",
    "ALL_RESULTS_JSON_FILE = '../results/services-range/all_results.json'\n",
    "SAVE_TO_JSON = False\n",
    "DATASET = 'inmates_enriched_10k'\n",
    "METRIC_NAME = 'qualitative'\n",
    "EXP_NUMBER = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27fea4-801d-4d2e-a92e-a491af7f44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FROM_JSON:\n",
    "    '''\n",
    "    with closing(sqlite3.connect(\"../db/remote-simulations-sadegh.db\")) as connection:\n",
    "        with closing(connection.cursor()) as cursor:\n",
    "            rows = cursor.execute(\"SELECT * from results\").fetchall()\n",
    "            print(rows)\n",
    "    '''\n",
    "    with closing(sqlite3.connect(\"../db/remote-simulations-sadegh.db\")) as connection_sadegh, \\\n",
    "            closing(sqlite3.connect(\"../db/remote-simulations.db\")) as connection_up, \\\n",
    "            closing(sqlite3.connect(\"../db/remote-simulations-vmware.db\")) as connection_vmware:\n",
    "        sql_query = pd.read_sql_query(\"SELECT * from results\", connection_sadegh)\n",
    "        results_sadegh = pd.DataFrame(sql_query)\n",
    "        sql_query = pd.read_sql_query(\"SELECT * from results\", connection_up)\n",
    "        results_up = pd.DataFrame(sql_query)\n",
    "        sql_query = pd.read_sql_query(\"SELECT * from results\", connection_vmware)\n",
    "        results_vmware = pd.DataFrame(sql_query)\n",
    "    \n",
    "    results = pd.concat([results_sadegh, results_up, results_vmware])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5c62c-8389-45c9-94f9-96c150153836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(metric_values):\n",
    "    return list([1 - x for x in metric_values])\n",
    "\n",
    "def normalize(metric_values):\n",
    "    values = invert(metric_values)\n",
    "    # the value becoming 1\n",
    "    highest = None if len(values) == 0 else values[-1]\n",
    "    \n",
    "    return list([x / highest for x in values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffec24c-c294-4347-93f6-e40c907555f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different quality metrics\n",
    "exp_params = {\n",
    "    1: {\n",
    "        'NODES_COUNT': 7,\n",
    "        'SERVICES_COUNT': 5,\n",
    "        'METRIC_NAME': METRIC_NAME,\n",
    "        'ROW_LOWER_BOUND': 0.2,\n",
    "        'ROW_UPPER_BOUND': 1,\n",
    "        'COLUMN_LOWER_BOUND': 0.6,\n",
    "        'COLUMN_UPPER_BOUND': 0.9,\n",
    "        'FILTERING_TYPE': 'mixed'\n",
    "    },\n",
    "    2: {\n",
    "        'NODES_COUNT': 9,\n",
    "        'SERVICES_COUNT': 5,\n",
    "        'METRIC_NAME': METRIC_NAME,\n",
    "        'ROW_LOWER_BOUND': 0.2,\n",
    "        'ROW_UPPER_BOUND': 1,\n",
    "        'COLUMN_LOWER_BOUND': 0.6,\n",
    "        'COLUMN_UPPER_BOUND': 0.9,\n",
    "        'FILTERING_TYPE': 'mixed'\n",
    "    },\n",
    "    3: {\n",
    "        'NODES_COUNT': 7,\n",
    "        'SERVICES_COUNT': 5,\n",
    "        'METRIC_NAME': 'qualitative',\n",
    "        'DATASET': DATASET,\n",
    "        'FILTERING_TYPE': 'mixed'\n",
    "    },\n",
    "    4: {\n",
    "        'NODES_COUNT': 6,\n",
    "        'METRIC_NAME': 'qualitative',\n",
    "        'ROW_LOWER_BOUND': 0.2,\n",
    "        'ROW_UPPER_BOUND': 1,\n",
    "        'COLUMN_LOWER_BOUND': 0.6,\n",
    "        'COLUMN_UPPER_BOUND': 0.9,\n",
    "        'DATASET': 'inmates_enriched_10k',\n",
    "        'FILTERING_TYPE': 'mixed'\n",
    "    }\n",
    "}\n",
    "\n",
    "exp_options = {\n",
    "    1: {\n",
    "        \"compare_props\": ['dataset'],\n",
    "        'filters': [\n",
    "            lambda df: df['services_count'] == exp_params[1]['SERVICES_COUNT'],\n",
    "            lambda df: df['nodes_count'] == exp_params[1]['NODES_COUNT'],\n",
    "            lambda df: df['metric_name'] == exp_params[1]['METRIC_NAME'],\n",
    "            lambda df: df['row_lower_bound'] == exp_params[1]['ROW_LOWER_BOUND'],\n",
    "            lambda df: df['row_upper_bound'] == exp_params[1]['ROW_UPPER_BOUND'],\n",
    "            lambda df: df['column_lower_bound'] == exp_params[1]['COLUMN_LOWER_BOUND'],\n",
    "            lambda df: df['column_upper_bound'] == exp_params[1]['COLUMN_UPPER_BOUND'],\n",
    "            lambda df: df['filtering_type'] == exp_params[1]['FILTERING_TYPE'],\n",
    "        ],\n",
    "        'legenda': lambda prop_values, i: str(prop_values)[7:-1],\n",
    "        'metric_transformation': normalize\n",
    "    },\n",
    "    2: {\n",
    "        \"compare_props\": ['dataset'],\n",
    "        'filters': [\n",
    "            lambda df: df['services_count'] == exp_params[2]['SERVICES_COUNT'],\n",
    "            lambda df: df['nodes_count'] == exp_params[2]['NODES_COUNT'],\n",
    "            lambda df: df['metric_name'] == exp_params[2]['METRIC_NAME'],\n",
    "            lambda df: df['row_lower_bound'] == exp_params[2]['ROW_LOWER_BOUND'],\n",
    "            lambda df: df['row_upper_bound'] == exp_params[2]['ROW_UPPER_BOUND'],\n",
    "            lambda df: df['column_lower_bound'] == exp_params[2]['COLUMN_LOWER_BOUND'],\n",
    "            lambda df: df['column_upper_bound'] == exp_params[2]['COLUMN_UPPER_BOUND'],\n",
    "            lambda df: df['filtering_type'] == exp_params[2]['FILTERING_TYPE'],\n",
    "        ],\n",
    "        'legenda': lambda prop_values, i: str(prop_values)[7:-1],\n",
    "        'metric_transformation': normalize\n",
    "    },\n",
    "    3: {\n",
    "        \"compare_props\": ['row_lower_bound', 'row_upper_bound', 'column_lower_bound', 'column_upper_bound'],\n",
    "        'filters': [\n",
    "            lambda df: df['services_count'] == exp_params[3]['SERVICES_COUNT'],\n",
    "            lambda df: df['nodes_count'] == exp_params[3]['NODES_COUNT'],\n",
    "            lambda df: df['metric_name'] == exp_params[3]['METRIC_NAME'],\n",
    "            lambda df: df['dataset'] == exp_params[3]['DATASET'],\n",
    "            lambda df: df['filtering_type'] == exp_params[3]['FILTERING_TYPE'],\n",
    "        ],\n",
    "        'legenda': lambda prop_values, i: chr(ord('A') + i),\n",
    "        'metric_transformation': invert\n",
    "    },\n",
    "    4: {\n",
    "        \"compare_props\": ['services_count'],\n",
    "        'filters': [\n",
    "            lambda df: df['nodes_count'] == exp_params[4]['NODES_COUNT'],\n",
    "            lambda df: df['metric_name'] == exp_params[4]['METRIC_NAME'],\n",
    "            lambda df: df['row_lower_bound'] == exp_params[4]['ROW_LOWER_BOUND'],\n",
    "            lambda df: df['row_upper_bound'] == exp_params[4]['ROW_UPPER_BOUND'],\n",
    "            lambda df: df['column_lower_bound'] == exp_params[4]['COLUMN_LOWER_BOUND'],\n",
    "            lambda df: df['column_upper_bound'] == exp_params[4]['COLUMN_UPPER_BOUND'],\n",
    "            lambda df: df['dataset'] == exp_params[4]['DATASET'],\n",
    "            lambda df: df['filtering_type'] == exp_params[4]['FILTERING_TYPE'],\n",
    "        ],\n",
    "        'legenda': lambda prop_values, i: 'S=' + str(prop_values),\n",
    "        'metric_transformation': invert\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8a5ff-29b6-4207-b0ec-f330a8137421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operator\n",
    "experiment_params  = exp_params[EXP_NUMBER]\n",
    "experiment_options = exp_options[EXP_NUMBER]\n",
    "COMPARE_PROPS = experiment_options['compare_props']\n",
    "params = experiment_params\n",
    "\n",
    "if not LOAD_FROM_JSON:\n",
    "    import functools\n",
    "        \n",
    "    props_values = results[COMPARE_PROPS].value_counts().index.to_frame(index=False)\n",
    "    \n",
    "    results_filters = experiment_options['filters']\n",
    "    \n",
    "    def get_compare_props_filters(p_values):\n",
    "        result = []\n",
    "        for field in p_values._fields:\n",
    "            field_value = getattr(p_values, field)\n",
    "            result.append((lambda f, fv: lambda df: df[f] == fv)(field, field_value))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def apply_filters_to_dataset(df, filters):\n",
    "        compound_condition = functools.reduce(operator.and_, map(lambda filter: filter(df), filters))\n",
    "        return df[compound_condition]\n",
    "    \n",
    "    props_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43edb0d6-5650-4c2d-98f5-77bf274411a9",
   "metadata": {},
   "source": [
    "## Aggregate results by multiple dimension\n",
    "\n",
    "After filtering the results, they are grouped by `window_size` so that metric values with different `experiment_id` are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f853e-467d-4356-981e-3b075589e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = {}\n",
    "\n",
    "if LOAD_FROM_JSON:\n",
    "    with open(AGG_RESULTS_JSON_FILE, 'r') as f:\n",
    "        agg_results = json.load(f)\n",
    "\n",
    "    for p_values in agg_results:\n",
    "        agg_results[p_values] = dict(zip(\n",
    "            [int(k) for k in agg_results[p_values]['metric_value'].keys()],\n",
    "            experiment_options['metric_transformation'](agg_results[p_values]['metric_value'].values())\n",
    "        ))\n",
    "else:    \n",
    "    for p_values in props_values.itertuples(index=False):\n",
    "        filtered_results = apply_filters_to_dataset(results, results_filters + get_compare_props_filters(p_values)) \\\n",
    "            .groupby(['window_size'])[['metric_value']].mean() \\\n",
    "            .sort_values(by=['window_size', 'metric_value'])\n",
    "    \n",
    "        display(filtered_results)\n",
    "        agg_results[p_values] = filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16c0ea-3b08-491d-9915-e429d229f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TO_JSON:\n",
    "    def save_aggregated_results(agg_results):\n",
    "        return { str(k): v.to_dict() for k, v in agg_results.items() }\n",
    "        \n",
    "    with open('saved_agg.json', 'w') as f:\n",
    "        f.write(json.dumps(save_aggregated_results(agg_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc2136-d190-48d8-9760-4f5084167f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, (p_values, dresults) in enumerate(sorted(map(lambda x: (int(x[0]), x[1]), agg_results.items()), key=lambda x: x[0])):\n",
    "    if len(dresults) == 0:\n",
    "        continue\n",
    "    ax.plot(dresults.keys(), dresults.values(), label=experiment_options['legenda'](p_values, i))\n",
    "\n",
    "plot_title = ', '.join(map(lambda x: f'{x[0]}={x[1]}', params.items()))\n",
    "if not IS_FOR_PAPER:\n",
    "    ax.set_title(plot_title)\n",
    "ax.set_xlabel('Window size')\n",
    "ax.set_ylabel('Metric value')\n",
    "ax.grid(True)\n",
    "\n",
    "if IS_FOR_PAPER:\n",
    "    ax.legend(bbox_to_anchor=(1.1, 0.1), loc='lower center', borderaxespad=0)\n",
    "else:\n",
    "    ax.legend(bbox_to_anchor=(1.02, 0.1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "plt.savefig(f'agg_{\",\".join(COMPARE_PROPS)}.svg', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c48064-5420-45be-808a-06e5e233a1a8",
   "metadata": {},
   "source": [
    "## All experiments by Dataset\n",
    "\n",
    "Results for the same experiment id are not aggregated but have their own plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6162a9-1773-4c91-a292-23b8b64dfe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map = {}\n",
    "\n",
    "if LOAD_FROM_JSON:\n",
    "    with open(ALL_RESULTS_JSON_FILE, 'r') as f:\n",
    "        results_map = json.load(f)\n",
    "\n",
    "    for p_values_map in results_map:\n",
    "        for exp_id in results_map[p_values_map]:\n",
    "            results_map[p_values_map][exp_id] = dict(zip(\n",
    "                [x for x in results_map[p_values_map][exp_id]['window_size'].values()],\n",
    "                experiment_options['metric_transformation']([x for x in results_map[p_values_map][exp_id]['metric_value'].values()])\n",
    "            ))\n",
    "else:\n",
    "    for p_values in props_values.itertuples(index=False):\n",
    "        results_map[p_values] = {}\n",
    "        filtered_results = apply_filters_to_dataset(results, results_filters + get_compare_props_filters(p_values))\n",
    "        experiment_ids = filtered_results['experiment_id'].unique()\n",
    "        for exp_id in experiment_ids:\n",
    "            exp_id_results = filtered_results[filtered_results['experiment_id'] == exp_id][['window_size', 'metric_value']].drop_duplicates()\n",
    "            results_map[p_values][exp_id] = exp_id_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cacce-eac3-41ac-b1cd-18709710c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import functools\n",
    "\n",
    "def compute_diff_for_results_per_prop_reducer(acc: List[Tuple[float, float]], cur: float):\n",
    "    '''\n",
    "    acc: (metric difference with the previous window size, metric for the current window size)\n",
    "    '''\n",
    "    metric_diff = cur - acc[-1][1]\n",
    "    return acc + [(metric_diff, cur)]\n",
    "\n",
    "def compute_diff_for_results_per_prop(results_per_prop: Dict[str, Dict[int, float]]) -> List[List[float]]:\n",
    "    return np.array([\n",
    "        np.array(list(map(lambda x: x[0], functools.reduce(compute_diff_for_results_per_prop_reducer, \\\n",
    "                         list(exp_results.values())[1:], \\\n",
    "                         [(0, list(exp_results.values())[0])] \\\n",
    "                        )))) \\\n",
    "        for exp_results in results_per_prop.values()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2420a-052b-44ce-9c2e-067476301546",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TO_JSON:\n",
    "    def save_all_results(all_results):\n",
    "        return { str(k): { str(k1): v1.to_dict() for k1, v1 in v.items() } for k, v in all_results.items() }\n",
    "        \n",
    "    with open('saved_all.json', 'w') as f:\n",
    "        f.write(json.dumps(save_all_results(results_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88b6d5-8bfb-48b2-955e-36262115163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_plot_settings(ax, title):\n",
    "    ax.set_xlabel('Window size')\n",
    "    ax.set_ylabel('Metric value')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    \n",
    "\n",
    "results_map = {k:v for k, v in results_map.items() if len(v) > 0}\n",
    "# 1. a boxplot\n",
    "# 2. a boxplot where elements are the difference between the metric of winsize `x` and winsize `x - 1`\n",
    "# 3. one plot that shows all the plot in the same subfigure\n",
    "additional_plot_count = 3\n",
    "fig_width = 9 if IS_FOR_PAPER else (max(len(exp_res) + additional_plot_count for exp_res in results_map.values()) * 4)\n",
    "\n",
    "results_count = len(results_map)\n",
    "fig_height = results_count * 5\n",
    "fig = plt.figure(figsize=(fig_width, fig_height), constrained_layout=True)\n",
    "\n",
    "plot_title = ', '.join(map(lambda x: f'{x[0]}={x[1]}', params.items()))\n",
    "if not IS_FOR_PAPER:\n",
    "    fig.suptitle(plot_title)\n",
    "\n",
    "subfigs = fig.subfigures(nrows=results_count, ncols=1, squeeze=False)\n",
    "\n",
    "for i, (p_values, results_per_prop) in enumerate(sorted(results_map.items(), key=lambda x: x[0])):\n",
    "    subfig = subfigs.item(i)\n",
    "    subfig.suptitle(f'Results for {str(p_values)[6:]}')\n",
    "\n",
    "    axs_len = 2 if IS_FOR_PAPER else (len(results_per_prop) + additional_plot_count)\n",
    "    axs = subfig.subplots(1, axs_len, sharex=True, squeeze=True)\n",
    "    min_y = min(min(results_per_prop[k].values()) for k in results_per_prop.keys())\n",
    "    max_y = max(max(results_per_prop[k].values()) for k in results_per_prop.keys())\n",
    "    # this offset let the plot to be slightly distant from the figure border\n",
    "    y_offset = (max_y - min_y) / 15\n",
    "    for i, ax in enumerate(axs):\n",
    "        if i != 1:\n",
    "            ax.set_ylim(min_y - y_offset, max_y + y_offset)\n",
    "\n",
    "    # additional_plot 1\n",
    "    axs[0].boxplot([[results_per_prop[exp_id][win_size] for exp_id in results_per_prop.keys()] \n",
    "                   for win_size in list(results_per_prop.items())[0][1].keys()])\n",
    "    prop_agg_results = agg_results[p_values]\n",
    "    axs[0].plot(list(prop_agg_results.keys()), list(prop_agg_results.values()), label=str(p_values)[7:-1])\n",
    "    set_plot_settings(axs[0], f'Boxplot with average')\n",
    "\n",
    "    # additional_plot 2\n",
    "    axs[1].boxplot(compute_diff_for_results_per_prop(results_per_prop))\n",
    "    set_plot_settings(axs[1], f'Boxplot for metric difference between winsizes')\n",
    "\n",
    "    if not IS_FOR_PAPER:\n",
    "        for exp_index, (exp_id, exp_id_results) in enumerate(results_per_prop.items()):\n",
    "            # additional_plot 3\n",
    "            axs[2].plot(exp_id_results['window_size'], exp_id_results['metric_value'])\n",
    "            set_plot_settings(axs[2], f'All experiments')\n",
    "            axs[exp_index + additional_plot_count].plot(exp_id_results['window_size'], exp_id_results['metric_value'])\n",
    "            set_plot_settings(axs[exp_index + additional_plot_count], f'Experiment id = {exp_id}')\n",
    "\n",
    "#fig.tight_layout()\n",
    "plt.savefig(f'all_exp_{\",\".join(COMPARE_PROPS)}.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
